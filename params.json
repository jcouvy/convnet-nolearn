{
  "name": "Experimenting convolutional networks with nolearn",
  "tagline": "><>)))]°>  ><>)))]°>  ><>)))]°>  ><>)))]°>  ><>)))]°>  ><>)))]°>  ><>)))]°>  ><>)))]°>  ><>)))]°>  ><>)))]°>  ><>)))]°>  ><>)))]°>  ",
  "body": "### Who am I ?\r\nI am a french undergraduate student in CS at the University of Bordeaux trying to learn more about _machine learning_,  _security_. This blog aims to help beginners like me, eager to take their first steps in machine learning. I am by no mean an expert of the field (I recommend you to peak at the urls given throughout the tutorial) nor is English my first language so please consider my work with a that in mind.\r\n\r\n## Table of Contents\r\n====================\r\n  + [Prerequisites](#prerequisites)\r\n  + [Introduction](#introduction)\r\n  + [Let's make a first model](#let's-make-a-first-model)\r\n  + [Loading a data-set](#loading-a-data-set)\r\n  + [Choosing a learning method](#choosing-a-learning-method)\r\n  + [Building a proper architecture](#building-a-proper-architecture)\r\n  + [Fine tuning parameters](#how-to-tune-the-number-of-kernels-(filters)-?)\r\n  + [Dealing with overfitting](#the-overfitting-problem)\r\n  + [Further improvements](#further-improvements)\r\n  + [Toolbox: how to visualize your data](#how-to-visualize-your-data-?)\r\n  + [Pro-tip: speed-up your network](#a-quick-tip-to-speed-up-your-net)\r\n\r\n## Introduction\r\n___\r\n\r\nIf you don't know anything about neural nets, take some time to look at this YouTube WebTV. It will help you fully understand the basis of deep learning _(all the credits goes to [DeepLearning.TV YT channel](https://www.youtube.com/channel/UC9OeZkIwhzfv-_Cb7fCikLQ/about))_:\r\n\r\n[![DLsimplified](https://cloud.githubusercontent.com/assets/19994887/16415171/37695a76-3d3c-11e6-9494-2732b7379e06.png)](https://www.youtube.com/watch?v=b99UVkWzYTQ&list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu \"Click me !\")\r\n\r\n## Prerequisites\r\n___\r\nIf you want to work on your own you will need to prepare your machine learning toolbox:\r\n+ [Python](http://www.python.org/) 2 >=2.6 or [Python](http://www.python.org/) 3 >=3.3 (we are using Python2.7)\r\n+ Set up a [virtualenv](https://virtualenv.pypa.io/en/stable/) (optional but recommended to keep things stable as nolearn is young and frequently updated)\r\n+ The [nolearn framework](https://github.com/dnouri/nolearn)\r\n+ A text editor of your choice: [GNU Emacs](https://www.gnu.org/software/emacs/) for example (I don't want to start a war here (◕‿◕✿))\r\n+ Access to a GPU with [CUDA](https://developer.nvidia.com/cuda-zone) (you can try simple nets with your CPU only)\r\n\r\n## Let's make a first model\r\n___\r\nWe will start to fiddle with the different layers available. This is where `nolearn` really shines as it allows a clear and easy way to implement your net. First we need to import a few modules:\r\n```python\r\nimport lasagne\r\nfrom lasagne import layers #this allows us to use each pre-defined layer.\r\nfrom nolearn.lasagne import NeuralNet #used to implement your net in a very simple fashion.\r\n```\r\nWe need to initialize our network just like a variable, let's name it `net0`. The initialization is divided in two steps:\r\n- Describe the network architecture (i.e: layer composition)\r\n- Initialize each layer's parameters\r\n\r\n```python\r\nnet0 = NeuralNet(\r\n    layers = [('layer1_name', layers.InsertLayerType),\r\n              ('layer2_name', layers.InsertLayerType),\r\n              (...)],\r\n    layer1_param1 = ..., layer1_param2 = ...,\r\n    layer2_param1 = ..., layer2_param2 = ...,\r\n)\r\n```\r\n**Bear in mind that you need to use the pre-defined parameters names or an error will occur**\r\n\r\nYou are probably not yet familiar with each different layers, no worries we are going to cover that right now. Every single available layer is already implemented in Lasagne and that's good news. Despite being young its documentation is rich and the community is helpful, the following link will get you to the [Lasagne official documentation](https://lasagne.readthedocs.io/en/latest/modules/layers.html) where each layer is described.\r\n\r\nNow that we have the doc opened we can start playing with `net0` :\r\n```python\r\n#from lasagne.updates import sgd\r\n\r\nnet0 = NeuralNet(\r\n    layers = [('input', layers.InputLayer),\r\n              ('conv1', layers.Conv2DLayer),\r\n              ('conv2', layers.Conv2DLayer),\r\n              ('output', layers.DenseLayer)\r\n    ],\r\n    input_shape = (None, 1, 28, 28), \r\n    \r\n    conv1_num_filters =  8,  conv1_filter_size = (3, 3),\r\n    conv2_num_filters =  16, conv2_filter_size = (2, 2),\r\n    \r\n    output_num_units = 10, output_nonlinearity = lasagne.nonlinearities.softmax,\r\n\r\n    #update = sgd,\r\n    #update_learning_rate = 0.01,\r\n    \r\n    max_epochs = 20,\r\n    verbose = 1,\r\n)\r\n```\r\nSo what do we have here...\r\n- `input_shape` describes the data processed by the network. Later on we will use a handwritten digit database named MNIST (you can find more about it [here](http://yann.lecun.com/exdb/mnist/)). Each digit is a 28x28 1-dimensionnal image (grayscale so only 1 color channel).  \r\nNB: the first parameter indicates the size of the batch, `None` means it is not fixed at compile time, thus `nolearn` will figure the value on its own.\r\n- `conv_num_filters` is the amount of filters (or kernels) you will use and `conv_filter_size` specifies their size.\r\n- `output_num_units` means that you sub-sample your data down to 10 images.\r\n- `output_nonlinearity` is the activation function used, here it's the maximum (most commonly used for classification problems).\r\n\r\nPicking the proper sizes seems a little mystical but we will get to that. A nice visualization of the filtering process can be found in a JavaScript implementation [here](http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html) (credits to @Karpathy).\r\n\r\nNow that we have a functional implementation we have to train it !\r\n\r\n## Loading a data-set\r\n___\r\nWe are going to use the set provided in the [Kaggle MNIST competition](https://www.kaggle.com/c/digit-recognizer).\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# The competition datafiles are in the directory ../input\r\n# Read training and test data files\r\ntrain = pd.read_csv(\"input/train.csv\")\r\ntest  = pd.read_csv(\"input/test.csv\")\r\n\r\ntrain_images = train.iloc[:,1:].values\r\ntrain_labels = train[[0]].values.ravel()\r\n```\r\nOnce the data-set is loaded we have to add a few lines to start the training: `trainX` is the data used to learn and `trainY` the expected result. This method is called « supervised learning ».\r\n\r\n```python\r\nnet0 = NeuralNet(\r\n    #...\r\n)\r\n# Reshape and normalize training data\r\ntrainX = train_images.reshape(train.shape[0], 1, 28, 28).astype(np.float32)\r\ntrainX /= 255.0\r\n\r\n# Reshape and normalize test data\r\ntestX = test.values.reshape(test.shape[0], 1, 28, 28).astype(np.float32)\r\ntestX /= 255.0\r\n\r\nnet0.fit(trainX, trainY)\r\n```\r\n**Uncomment the lines in** `net0` **if you want to test it yourself**\r\n\r\nThis is the results after 20 epochs (the input display origins from `verbose=1`):\r\n```bash\r\n# Neural Network with 100618 learnable parameters\r\n\r\n## Layer information\r\n\r\n  #  name    size\r\n---  ------  --------\r\n  0  input   1x28x28\r\n  1  conv1   8x26x26\r\n  2  conv2   16x25x25\r\n  3  output  10\r\n\r\n  epoch    train loss    valid loss    train/val    valid acc  dur\r\n-------  ------------  ------------  -----------  -----------  ------\r\n      1       5.16510       0.42123     12.26191      0.86043  10.53s\r\n      2       0.35298       0.30983      1.13928      0.89896  10.58s\r\n      3       0.27154       0.27564      0.98513      0.91174  10.55s\r\n      4       0.23111       0.26905      0.85899      0.91618  10.59s\r\n      5       0.20591       0.26782      0.76883      0.91595  10.71s\r\n      6       0.18814       0.27432      0.68583      0.91678  10.70s\r\n      7       0.17368       0.27859      0.62342      0.91891  10.56s\r\n      8       0.16238       0.28430      0.57115      0.91926  10.69s\r\n      9       0.15185       0.30812      0.49283      0.91541  10.67s\r\n     10       0.14281       0.32322      0.44183      0.91465  10.61s\r\n     11       0.13528       0.33334      0.40583      0.91328  10.59s\r\n     12       0.12717       0.33398      0.38077      0.91358  10.66s\r\n     13       0.12007       0.33142      0.36229      0.91678  10.63s\r\n     14       0.11403       0.33387      0.34153      0.91666  10.61s\r\n     15       0.10853       0.34899      0.31097      0.91571  10.56s\r\n     16       0.10315       0.36111      0.28566      0.91334  10.58s\r\n     17       0.10049       0.35958      0.27947      0.91494  10.56s\r\n     18       0.09618       0.37745      0.25481      0.91370  10.58s\r\n     19       0.09219       0.38261      0.24093      0.91322  10.63s\r\n     20       0.08899       0.41470      0.21458      0.91062  10.68s\r\n```\r\nHuman performance would be above 98%, besides a network is considered industry-viable if its accuracy is at least 95%. We're not there yet but `net0` is really basic.\r\n\r\n## Choosing a learning method\r\n___\r\nWe just learned that the origins of artificial neural networks are found in biomimetism. Our goal is to build a network able to learn like a brain would do. However not every problem can be practically adapted into code by a programmer, this is where learning algorithms become useful. There are three major learning paradigms :\r\n+ Supervised learning (the one we will focus on)\r\n+ Unsupervised learning\r\n+ Reinforcement learning\r\n\r\nThere are plenty of information about these methods elsewhere on the web or in your favorite library so we won't cover them up here. The following graph is a comparison of the commonly used supervised learning methods:\r\n\r\n![learningmethod](https://cloud.githubusercontent.com/assets/19994887/16196236/91019214-36fd-11e6-9e9b-1113334e7261.png)\r\n\r\nOur network will use `nesterov momentum` for two reasons, firstly it has good performance and secondly it requires to specify both a learning rate and momentum values (this will be helpful later).\r\n\r\n## Building a proper architecture\r\n___\r\nTo obtain better results than `net0` we are going to need more than convolution layers. Most ConvNets are constituted by a succession of different layers following a specific pattern (see [the course CS231n from Stanford University](http://cs231n.github.io/convolutional-networks/#layerpat)):\r\n\r\n>The most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with POOL layers, and repeats this pattern until the image has been merged spatially to a small size. At some point, it is common to transition to fully-connected layers. The last fully-connected layer holds the output, such as the class scores. In other words, the most common ConvNet architecture follows the pattern:  \r\n\r\n`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`\r\n\r\n>Where the `*` indicates repetition, and the `POOL?` indicates an optional pooling layer. Moreover, `N >= 0` (and usually `N <= 3`), `M >= 0`, `K >= 0` (and usually `K < 3`).\r\n\r\nConsidering all this we will add a few layers to `net0`:\r\n\r\n```python\r\nnet1 = NeuralNet(\r\n    layers = [('input', layers.InputLayer),\r\n              ('conv1', layers.Conv2DLayer),\r\n              ('pool1', layers.MaxPool2DLayer),\r\n              ('conv2', layers.Conv2DLayer),\r\n              ('pool2', layers.MaxPool2DLayer),\r\n              ('hidden', layers.DenseLayer),\r\n              ('output', layers.DenseLayer)\r\n              ],\r\n              # (...)\r\n)\r\n```\r\n```bash\r\n# Neural Network with 38186 learnable parameters\r\n\r\n  epoch    train loss    valid loss    train/val    valid acc  dur\r\n-------  ------------  ------------  -----------  -----------  -----\r\n      1       1.02026       0.25914      3.93707      0.91891  7.89s\r\n     ..       .......       .......      .......      .......  .....\r\n     10       0.05761       0.08531      0.67536      0.97561  7.88s\r\n     ..       .......       .......      .......      .......  .....\r\n     20       0.02686       0.10420      0.25778      0.97182  7.91s\r\n     ..       .......       .......      .......      .......  .....\r\n     30       0.01065       0.11439      0.09308      0.97691  8.01s\r\n     ..       .......       .......      .......      .......  .....\r\n     40       0.00297       0.14241      0.02082      0.97573  8.04s\r\n     ..       .......       .......      .......      .......  .....\r\n     50       0.00100       0.16171      0.00615      0.97615  7.99s\r\n```\r\nWe notice a few things:\r\n* Lowered amount of learnable parameters (due to the pooling layers).\r\n* The network is a lot faster (consequence of the above). The 2 seconds difference might not feel important to you but a deeper network can be trained for over 3000 epochs. \r\n* Much better accuracy >97%.\r\n\r\nLet's trace a graph using `pyplot` to compare our results:\r\n\r\n![Net0 vs Net1](https://cloud.githubusercontent.com/assets/19994887/16229020/c9d4c666-37ba-11e6-8498-f8c7176c1dec.png)\r\n\r\nBoth `net1` curves drop significantly lower, its a sign we're improving !\r\n\r\n## How to tune the number of kernels (filters) ?\r\n___\r\nNow this is a tricky part... From what I have read and learned it is mostly done empirically. One could say it is \"half intuition, half black magic\". A team of Google researchers tried to explain the importance of proper initialization and gave some guidelines, see [Rethinking the Inception Architecture for Computer Vision](http://arxiv.org/abs/1512.00567).\r\n\r\n For the moment a simple way of proceeding is, firstly, to enable `verbose=2` (it will give us more information about our network's learning capacity of the coverage of its data) then fix a small amount of iteration `max_epochs=10` and test the best combination of kernel sizes (best ratio accuracy/duration for example). You may want to cover close to 100% of the image, however as MNIST digits don't cover the whole 28*28 pixels it is acceptable to have a lower coverage value.\r\n\r\nLets take a look at the input of `verbose=2` :\r\n```bash\r\n# Neural Network with 38186 learnable parameters\r\n\r\n## Layer information\r\n\r\nname    size        total    cap.Y    cap.X    cov.Y    cov.X\r\n------  --------  -------  -------  -------  -------  -------\r\ninput   1x28x28       784   100.00   100.00   100.00   100.00\r\nconv1   8x26x26      5408   100.00   100.00    10.71    10.71\r\npool1   8x13x13      1352   100.00   100.00    10.71    10.71\r\nconv2   16x12x12     2304    80.00    80.00    17.86    17.86\r\npool2   16x6x6        576    80.00    80.00    17.86    17.86\r\nhidden  64             64   100.00   100.00   100.00   100.00\r\noutput  10             10   100.00   100.00   100.00   100.00\r\n\r\nExplanation\r\n    X, Y:    image dimensions\r\n    cap.:    learning capacity\r\n    cov.:    coverage of image\r\n    magenta: capacity too low (<1/6)\r\n    cyan:    image coverage too high (>100%)\r\n    red:     capacity too low and coverage too high\r\n\r\n```\r\nSadly we cannot see the color scheme here, there is no problem on a terminal though. The output will guide you through a simple optimization of your values. Imagine a network composed of multiple pooling layers, the output would be as follows:\r\n```bash\r\nname    size       total    cap.Y    cap.X    cov.Y    cov.X\r\n------  -------  -------  -------  -------  -------  -------\r\ninput   1x28x28      784   100.00   100.00   100.00   100.00\r\nconv1   8x28x28     6272   100.00   100.00    10.71    10.71\r\npool1   8x14x14     1568   100.00   100.00    10.71    10.71\r\nconv2   8x14x14     1568    85.71    85.71    25.00    25.00\r\npool2   8x7x7        392    85.71    85.71    25.00    25.00\r\nconv3   16x7x7       784    80.00    80.00    53.57    53.57\r\npool3   16x3x3       144    80.00    80.00    53.57    53.57\r\nconv4   16x3x3       144    77.42    77.42   110.71   110.71\r\npool4   16x1x1        16    77.42    77.42   110.71   110.71\r\noutput  10            10   100.00   100.00   100.00   100.00\r\n```\r\nEach pooling layer is sub-sampling the input, the resulting image is really small. On the 4th convolution the network would cover more than 100% of the image surface (should appear in cyan), this is a pretty bad optimization.\r\n\r\n## The overfitting problem\r\n___\r\nTo understand the issue we need to scroll back to the `net1` graph. Even though the results are better we can clearly see the `train/loss` value (depicted in red) increase while the `valid/loss` steadily decreases. This is often a situation where overfitting have occured. But what is « overfitting » ?\r\n\r\n> In statistics and machine learning, one of the most common tasks is to fit a \"model\" to a set of training data, so as to be able to make reliable predictions on general untrained data. In overfitting, a statistical model describes **random error** or **noise** instead of the underlying relationship. Overfitting occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. A model that has been overfit has **poor predictive performance**, as it overreacts to minor fluctuations in the training data.  \r\n_(source: https://en.wikipedia.org/wiki/Overfitting)_\r\n\r\nThe definition might not be clear to everyone, focus on the highlighted parts. We can easily assume two things: \r\n+ Overfitting is a common issue in ML.\r\n+ We want to get rid of it as best as we can.\r\n\r\nThere are different ways to deal with overfitting, in `net2` we will try to use the `BatchIterator` provided by `nolearn` to simulate a larger amount of data. This technique is called **Data augmentation**. The general idea is apply on-the-fly modifications (thanks to `BatchIterator`) to your data-set without altering the labels. Here we will randomly crop the image by 2 pixels.\r\n\r\n```python\r\nfrom nolearn.lasagne import BatchIterator, #...\r\n\r\n# Custom Batch Iterator called by NeuralNet to provided augmented\r\n# data in order to reduce overfitting.\r\n# Randomly crops by 2 pixels the inputed image.\r\n# Returns the new tensor Xb (trainX) and the labels.\r\nclass CropBatchIterator(BatchIterator):\r\n    cropX, cropY = 2, 2\r\n    def transform(self, Xb, yb):\r\n        Xb, yb = super(CropBatchIterator, self).transform(Xb, yb)\r\n        shape = Xb.shape[0]\r\n        width = 28 - self.cropX\r\n        height = 28 - self.cropY\r\n        new_Xb = np.zeros([shape, 1, width, height], dtype=np.float32)\r\n        for i in range (shape):\r\n            dx = np.random.randint(self.cropX+1)\r\n            dy = np.random.randint(self.cropY+1)\r\n            new_Xb[i] = Xb[i, :, dy:dy+width, dx:dx+height]\r\n        return new_Xb, yb\r\n\r\n```\r\n\r\nWe need to add a few lines to our network to make it work properly. You might notice the architecture changed a little, this is what I found to be a decent set of parameters (see [Fine tuning parameters](#how-to-tune-the-number-of-kernels-(filters)-?))\r\n\r\n```python\r\nnet2 = NeuralNet(\r\n    layers = [('input', layers.InputLayer),\r\n              ('conv1', layers.Conv2DLayer),\r\n              ('pool1', layers.MaxPool2DLayer),\r\n              ('conv2', layers.Conv2DLayer),\r\n              ('pool2', layers.MaxPool2DLayer),\r\n              ('hidden1', layers.DenseLayer),\r\n              ('hidden2', layers.DenseLayer),\r\n              ('output', layers.DenseLayer)\r\n              ],\r\n    # Each digit is a 28x28 1-Dimensionnal image,\r\n    # We crop each image by 2 pixels in width and height.\r\n    input_shape = (None, 1, 26, 26),\r\n\r\n    conv1_num_filters =  13, conv1_filter_size = (3, 3),\r\n    conv1_nonlinearity = lasagne.nonlinearities.rectify, #ReLu rectifier (default value)\r\n    pool1_pool_size = (2, 2),\r\n    \r\n    conv2_num_filters = 26, conv2_filter_size = (2, 2),\r\n    conv2_nonlinearity = lasagne.nonlinearities.rectify,\r\n    pool2_pool_size = (2, 2),\r\n\r\n    hidden1_num_units = 128, hidden2_num_units = 128,\r\n    output_num_units = 10, \r\n    output_nonlinearity = lasagne.nonlinearities.softmax, #Softmax classifier (default value)\r\n\r\n    # Learning method.\r\n    update = nesterov_momentum,\r\n    update_learning_rate = 0.01,\r\n    update_momentum = 0.9,    \r\n\r\n    # Calls the class CropBatchIterator while training occurs\r\n    # Doesn't affect GPU performance as the CPU deals with the\r\n    # cropping operation.\r\n    batch_iterator_train = CropBatchIterator(batch_size=200),\r\n    batch_iterator_test = CropBatchIterator(batch_size=200),\r\n\r\n    regression = False,\r\n    max_epochs = 500,\r\n\r\n    # Deeper layer information (learning capacity, coverage...)\r\n    verbose = 2, \r\n)\r\n```\r\n![net1net2](https://cloud.githubusercontent.com/assets/19994887/16303398/bb9328a8-3950-11e6-8185-547eb1e7cd0c.png)\r\n\r\nWe also manage to obtain better results with ~98.7% on average, however the network still tends to be overfitting a little (the reason is probably that MNIST is too simple).\r\n\r\n```bash\r\n epoch    train loss    valid loss    train/val    valid acc  dur\r\n-------  ------------  ------------  -----------  -----------  -----\r\n      1       1.18662       0.40577      2.92434      0.87733  8.72s\r\n     ..       .......       .......      .......      .......  .....\r\n     50       0.01686       0.04535      0.37183      0.98744  8.79s\r\n     ..       .......       .......      .......      .......  .....\r\n    100       0.00561       0.05057      0.11096      0.98733  8.82s\r\n     ..       .......       .......      .......      .......  .....\r\n    200       0.00095       0.06931      0.01376      0.98721  8.81s\r\n     ..       .......       .......      .......      .......  .....\r\n    300       0.00007       0.06970      0.00101      0.98930  8.76s\r\n     ..       .......       .......      .......      .......  .....\r\n    400       0.00004       0.07738      0.00045      0.99000  8.70s\r\n     ..       .......       .......      .......      .......  .....\r\n    450       0.00003       0.06610      0.00039      0.99000  8.83s\r\n     ..       .......       .......      .......      .......  .....\r\n    500       0.00002       0.07326      0.00030      0.98884  8.71s\r\n\r\n```\r\n\r\n## Further improvements\r\n\r\nWe can still scratch a little more performance while reducing validation loss with simple changes. First we will need to add another type of layers called `Dropout`.\r\n> Dropout is a technique that (...) prevents overfitting and\r\nprovides a way of approximately combining exponentially many different neural network\r\narchitectures efficiently. The term “dropout” refers to dropping out units (hidden and\r\nvisible) in a neural network. By dropping a unit out, we mean temporarily removing it from\r\nthe network, along with all its incoming and outgoing connections, as shown in Figure (a).\r\nThe choice of which units to drop is random. In the simplest case, each unit is retained with\r\na fixed probability p independent of other units, where p can be chosen using a validation\r\nset or can simply be set at 0.5, which seems to be close to optimal for a wide range of\r\nnetworks and tasks (...).\r\n_See_ [_(Dropout: A Simple Way to Prevent Neural Networks from Overfitting)_](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\r\n\r\n![dropoutlayer](https://cloud.githubusercontent.com/assets/19994887/16375020/a186beaa-3c59-11e6-892e-bf59d565257e.png)\r\n\r\n```python\r\nnet3 = NeuralNet(\r\n    layers = [('input', layers.InputLayer),\r\n              #(...),\r\n              ('dropout', layers.DropoutLayer),\r\n              #(...)\r\n              ],\r\n    dropout_p = 0.3,\r\n```\r\n\r\nThis is not the only change we will do to `net2`, remember why we decided to pick `nesterov momemtum` as learning method ? Optimizations that is ! What's good about the method is that it needs to have a fixed learning rate and momentum. What's even better is that `nolearn` allows us to dynamically change these values as the training occurs. We will create a custom class charged to update the value:\r\n\r\n```python\r\n# Converts numbers into 32b floats best used w/ GPUs.\r\ndef float32(k):\r\n    return np.cast['float32'](k)\r\n\r\n# Allows fine tuning of hyper-parameters during the learning process\r\n# Credits to (@Karpathy)\r\nclass AdjustVariable(object):\r\n    def __init__(self, name, start=0.03, stop=0.001):\r\n        self.name = name\r\n        self.start = start\r\n        self.stop = stop\r\n        self.ls = None\r\n\r\n    def __call__(self, nn, train_history):\r\n        if self.ls is None:\r\n            self.ls = np.linspace(self.start, self.stop, nn.max_epochs)\r\n\r\n        epoch = train_history[-1]['epoch']\r\n        new_value = float32(self.ls[epoch - 1])\r\n        getattr(nn, self.name).set_value(new_value)\r\n```\r\nWhen called, `AdjustVariable` updates the parsed variable with its new value. But how does it work exactly ?  \r\n  `np.linspace` is a numpy function used to create a 1D array ranging from [start, stop] of length max_epochs so that the new values are evenly distributed no matter the duration. `nolearn` allows us to search values in the history of the training with `train_history`. We save the latest `epoch` value and set the new values correspondingly using the Python built-in function `getattr`. Note that we need to convert the value to a GPU oriented format `np.float32`.\r\n\r\nNow that we're done with the class we have to inform `net3` that it needs to use our brand new class, with `nolearn` we can do it pretty easily:\r\n```python\r\n    on_epoch_finished=[\r\n        AdjustVariable('update_learning_rate', start=0.03, stop=0.0001),\r\n        AdjustVariable('update_momentum', start=0.9, stop=0.999),\r\n        ],\r\n```\r\n\r\nThere is one last change to make before fitting our net. In order to dynamically change `update_learning_rate` and `update_momentum` we have to change the variable type to a Theano-type named `shared variable`. We never spoke of Theano as it is not the purpose of this write-up however I will briefly explain what it is:\r\n> Theano is a Python library that lets you to define, optimize, and evaluate mathematical expressions, especially ones with multi-dimensional arrays (numpy.ndarray). Using Theano it is possible to attain speeds rivaling hand-crafted C implementations for problems involving large amounts of data.  \r\n_(source: http://deeplearning.net/software/theano/introduction.html#introduction)_\r\n\r\nWe saw that Nolearn is built on top of Lasagne right ? Well, Lasagne is to Theano as Nolearn is to Lasagne ! Nolearn's goal is to abstract as much as possible every concept in order to keep things simple and clear. Theano however, only wanted to leave most of the tedious mathematical part hidden. While Nolearn might be simpler to understand for beginners, Theano allows much more customization (such as what we're trying to achieve). Thankfully since Nolearn is a wrapper built around Theano, we can use both in the same program.  \r\n\r\nA `shared variable` is a « Variable with Storage that is shared between functions that it appears in », Theano uses a lot of symbolic expressions (see: [S-expression](https://en.wikipedia.org/wiki/S-expression)) as it helps generating C  code.\r\n```python\r\nimport theano\r\n# (...)\r\nupdate_learning_rate = theano.shared(float32(0.03)),\r\nupdate_momentum = theano.shared(float32(0.9)), \r\n```\r\nLet's now compare `net2` and `net3`:\r\n\r\n![net2net3](https://cloud.githubusercontent.com/assets/19994887/16452831/c9130d9a-3e0a-11e6-9e8e-c9f3f8900340.png)\r\n\r\nWe notice increased training loss (due to dropout layers deactivating nodes), better accuracy, lower validation loss but also a more consistent learning potential (see `train/val` column in the following array). That means the network can still learn more ! While this is not a great deal on MNIST since we already have good results, it is really important on more complex problems.\r\n\r\n```bash\r\n  epoch    train loss    valid loss    train/val    valid acc  dur\r\n-------  ------------  ------------  -----------  -----------  -----\r\n      1       0.76528       0.21292      3.59415      0.92663  8.69s\r\n     ..       .......       .......      .......      .......  .....\r\n    100       0.01661       0.03804      0.43662      0.99012  8.70s\r\n     ..       .......       .......      .......      .......  .....\r\n    300       0.00601       0.04432      0.13559      0.99140  8.69s\r\n     ..       .......       .......      .......      .......  .....\r\n    400       0.00533       0.04996      0.10667      0.99058  8.70s\r\n     ..       .......       .......      .......      .......  .....\r\n    500       0.00488       0.04629      0.10548      0.99186  8.70s\r\n     ..       .......       .......      .......      .......  .....\r\n    600       0.00317       0.04891      0.06490      0.99163  8.69s\r\n     ..       .......       .......      .......      .......  .....\r\n    700       0.00317       0.04387      0.07222      0.99244  8.73s\r\n     ..       .......       .......      .......      .......  .....\r\n    800       0.00256       0.05217      0.04900      0.99105  8.70s\r\n     ..       .......       .......      .......      .......  .....\r\n    900       0.00227       0.04094      0.05544      0.99337  8.70s\r\n     ..       .......       .......      .......      .......  .....\r\n   1000       0.00163       0.05022      0.03253      0.99314  8.69s\r\n```\r\n\r\nThat's it we managed to build a convolutional network able to recognize hand-written digits with an accuracy of 99.3%, pretty close from human performance (one could say better).  \r\n\r\nI hope this walk through helped some of you \\\\(◕ ◡ ◕\\\\).\r\n\r\n### How to visualize your data ?\r\n___\r\n\r\n`nolearn` offers easy ways of representing the evolution of your network, finding errors ... \r\n\r\n```python\r\nfrom nolearn.lasagne.visualize import draw_to_file #draw_to_notebook if using IPython Notebook\r\nfrom nolearn.lasagne.visualize import plot_loss\r\nfrom nolearn.lasagne.visualize import plot_conv_weights\r\nfrom nolearn.lasagne.visualize import plot_conv_activity\r\nfrom nolearn.lasagne.visualize import plot_occlusion\r\nfrom nolearn.lasagne.visualize import plot_saliency\r\n```\r\n\r\nWith these you can simply add a few lines at the end of your program, say we want plot the validation loss of our last fitting. It is as simple as :\r\n```python\r\nplot_loss(network_name)\r\nplt.savefig(\"path_to_results/plotloss.png\")\r\n```\r\n\r\n\r\n\r\nThe [source code](https://github.com/dnouri/nolearn/blob/master/nolearn/lasagne/visualize.py) can be found on `nolearn` github repository.\r\n\r\n### A quick tip to speed up your net\r\n___\r\n\r\nThis is something that I found recently while browsing the Lasagne API, it offers some CUDA-specific re-implementation of some its layers (mostly for convolution nets, how fortunate !). Remember that `CUDA` requires you to use a GPU to train your network, using these layers will NOT work under any other circumstances.  \r\n\r\nIf you want to benefit from these layers without having to change your architecture, add these imports to your code:\r\n```python\r\ntry:\r\n    from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayer\r\n    from lasagne.layers.cuda_convnet import MaxPool2DCCLayer as MaxPool2DLayer\r\nexcept ImportError:\r\n    Conv2DLayer = layers.Conv2DLayer\r\n    MaxPool2DLayer = layers.MaxPool2DLayer\r\n```\r\nOtherwise head-on to [the documentation](https://lasagne.readthedocs.io/en/latest/modules/layers/cuda_convnet.html) and use the proper names.\r\n\r\n### Credits\r\nDaniel Nouri (@dnouri) author of nolearn and Andrej Karpathy (@karpathy) for his ML courses.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}