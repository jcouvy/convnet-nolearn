{
  "name": "Testing project site",
  "tagline": "This is literally a fish >)))]°>",
  "body": "Greetings, blablabla i'm a french student trying to speak decent english and write a blog about ML.\r\n\r\n## Prerequisites\r\nThis automatic page generator is the easiest way to create beautiful pages for all of your projects. Author your page content here [using GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/), select a template crafted by a designer, and publish. After your page is generated, you can check out the new `gh-pages` branch locally. If you’re using GitHub Desktop, simply sync your repository and you’ll see the new branch.\r\n\r\n## Introduction\r\nWe’ve crafted some handsome templates for you to use. Go ahead and click 'Continue to layouts' to browse through them. You can easily go back to edit your page before publishing. After publishing your page, you can revisit the page generator and switch to another theme. Your Page content will be preserved.\r\n\r\n### Convolutional network\r\n\r\n## Let's make a first model\r\nWe will start to fiddle with the different layers available. This is where `nolearn` really shines as it allows a clear and easy way to implement your net. First we need to import a few modules:\r\n```python\r\nimport lasagne\r\nfrom lasagne import layers #this allows us to use each pre-defined layer.\r\nfrom nolearn.lasagne import NeuralNet #used to implement your net in a very simple fashion.\r\n```\r\nWe need to initialize our network just like a variable, let's name it `net0`. The initialization is divided in two steps:\r\n- Describe the network architecture (i.e: layer composition)\r\n- Initialize each layer's parameters\r\n\r\n```python\r\nnet0 = NeuralNet(\r\n    layers = [('layer1_name', layers.InsertLayerType),\r\n              ('layer2_name', layers.InsertLayerType),\r\n              (...)],\r\n    layer1_param1 = ..., layer1_param2 = ...,\r\n    layer2_param1 = ..., layer2_param2 = ...,\r\n)\r\n```\r\n**Bear in mind that you need to use the pre-defined parameters name or an error will occur**\r\n\r\nYou are probably not yet familiar with each different layers, no worries we are going to cover that right now. Every single available layer is already implemented in Lasagne and that's good news. Despite being young its documentation is rich and the community is helpful, the following link will get you to the [Lasagne official documentation](https://lasagne.readthedocs.io/en/latest/modules/layers.html) where each layer is described.\r\n\r\nNow that we have the doc opened we can start playing with `net0` :\r\n```python\r\n#from lasagne.updates import sgd\r\n\r\nnet0 = NeuralNet(\r\n    layers = [('input', layers.InputLayer),\r\n              ('conv1', layers.Conv2DLayer),\r\n              ('conv2', layers.Conv2DLayer),\r\n              ('output', layers.DenseLayer)\r\n    ],\r\n    input_shape = (None, 1, 28, 28), \r\n    \r\n    conv1_num_filters =  8,  conv1_filter_size = (3, 3),\r\n    conv2_num_filters =  16, conv2_filter_size = (2, 2),\r\n    \r\n    output_num_units = 10, output_nonlinearity = lasagne.nonlinearities.softmax,\r\n\r\n    update = sgd,\r\n    update_learning_rate = 0.01,\r\n    \r\n    max_epochs = 20,\r\n    verbose = 1,\r\n)\r\n```\r\nSo what do we have here...\r\n- `input_shape` describes the data processed by the network. Later on we will use a handwritten digit database named MNIST (you can find more about it [here](http://yann.lecun.com/exdb/mnist/)). Each digit is a 28x28 1-dimensionnal image (grayscale so only 1 color channel).  \r\nNB: the first parameter indicates the size of the batch, `None` means it is not fixed at compile time, thus `nolearn` will figure the value on its own.\r\n- `conv_num_filters` is the amount of filters (or kernels) you will use and `conv_filter_size` specifies their size.\r\n- `output_num_units` means that you sub-sample your data down to 10 images.\r\n- `output_nonlinearity` is the activation function used, here it's the maximum (most commonly used for classification problems).\r\n\r\nPicking the proper sizes seems a little mystical but we will get to that. A nice visualization of the filtering process can be found in a JavaScript implementation [here](http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html) (credits to @Karpathy).\r\n\r\nNow that we have a functional implementation we have to train it !\r\n## Loading a data-set\r\nWe are going to use the set provided in the [Kaggle MNIST competition](https://www.kaggle.com/c/digit-recognizer).\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n# The competition datafiles are in the directory ../input\r\n# Read competition data files:\r\ntrain = pd.read_csv(\"input/train.csv\")\r\ntest  = pd.read_csv(\"input/test.csv\") # Not used during training\r\n\r\ntrain_images = train.iloc[:,1:].values\r\ntrain_labels = train[[0]].values.ravel()\r\n\r\n# Converting to numpy type (32bits for GPUs)\r\ntrain_images = np.array(train_images).astype(np.float32)\r\ntrain_labels = train_labels.astype(np.int32)\r\n```\r\nOnce the data-set is loaded we have to add a few lines to start the training: `trainX` is the data used to learn and `trainY` the expected result. This method is called «Supervised learning».\r\n\r\n```python\r\nnet0 = NeuralNet(\r\n    #...\r\n)\r\ntrainX = train_images.reshape(-1, 1, 28, 28)\r\ntrainY = train_labels\r\nnet0.fit(trainX, trainY)\r\n```\r\n**Uncomment the lines in** `net0` **if you want to test it yourself**\r\n\r\nThis is the results after 20 epochs (the input display origins from `verbose=1`):\r\n```\r\n# Neural Network with 100618 learnable parameters\r\n\r\n## Layer information\r\n\r\n  #  name    size\r\n---  ------  --------\r\n  0  input   1x28x28\r\n  1  conv1   8x26x26\r\n  2  conv2   16x25x25\r\n  3  output  10\r\n\r\n  epoch    train loss    valid loss    train/val    valid acc  dur\r\n-------  ------------  ------------  -----------  -----------  ------\r\n      1       5.16510       0.42123     12.26191      0.86043  10.53s\r\n      2       0.35298       0.30983      1.13928      0.89896  10.58s\r\n      3       0.27154       0.27564      0.98513      0.91174  10.55s\r\n      4       0.23111       0.26905      0.85899      0.91618  10.59s\r\n      5       0.20591       0.26782      0.76883      0.91595  10.71s\r\n      6       0.18814       0.27432      0.68583      0.91678  10.70s\r\n      7       0.17368       0.27859      0.62342      0.91891  10.56s\r\n      8       0.16238       0.28430      0.57115      0.91926  10.69s\r\n      9       0.15185       0.30812      0.49283      0.91541  10.67s\r\n     10       0.14281       0.32322      0.44183      0.91465  10.61s\r\n     11       0.13528       0.33334      0.40583      0.91328  10.59s\r\n     12       0.12717       0.33398      0.38077      0.91358  10.66s\r\n     13       0.12007       0.33142      0.36229      0.91678  10.63s\r\n     14       0.11403       0.33387      0.34153      0.91666  10.61s\r\n     15       0.10853       0.34899      0.31097      0.91571  10.56s\r\n     16       0.10315       0.36111      0.28566      0.91334  10.58s\r\n     17       0.10049       0.35958      0.27947      0.91494  10.56s\r\n     18       0.09618       0.37745      0.25481      0.91370  10.58s\r\n     19       0.09219       0.38261      0.24093      0.91322  10.63s\r\n     20       0.08899       0.41470      0.21458      0.91062  10.68s\r\n```\r\nHuman performance would be above 98%, besides a network is considered industry-viable if its accuracy is at least 95%. We're not there yet but `net0` is really basic.\r\n\r\n## Choosing a learning method\r\n![](<img src=\"img/net0_lrmethod.png\" alt=\"hi\" class=\"inline\"/>)\r\n\r\n\r\n\r\n### Authors and Contributors\r\nYou can @mention a GitHub username to generate a link to their profile. The resulting `<a>` element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (@defunkt), PJ Hyett (@pjhyett), and Tom Preston-Werner (@mojombo) founded GitHub.\r\n\r\n### Support or Contact\r\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/pages) or [contact support](https://github.com/contact) and we’ll help you sort it out.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}