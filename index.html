<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Testing project site by jcouvy</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Testing project site</h1>
      <h2 class="project-tagline">This is literally a fish &gt;)))]°&gt;</h2>
      <a href="https://github.com/jcouvy/convnet-nolearn" class="btn">View on GitHub</a>
      <a href="https://github.com/jcouvy/convnet-nolearn/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/jcouvy/convnet-nolearn/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>Greetings, blablabla i'm a french student trying to speak decent english and write a blog about ML.</p>

<h2>
<a id="prerequisites" class="anchor" href="#prerequisites" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Prerequisites</h2>

<p>This automatic page generator is the easiest way to create beautiful pages for all of your projects. Author your page content here <a href="https://guides.github.com/features/mastering-markdown/">using GitHub Flavored Markdown</a>, select a template crafted by a designer, and publish. After your page is generated, you can check out the new <code>gh-pages</code> branch locally. If you’re using GitHub Desktop, simply sync your repository and you’ll see the new branch.</p>

<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

<p>We’ve crafted some handsome templates for you to use. Go ahead and click 'Continue to layouts' to browse through them. You can easily go back to edit your page before publishing. After publishing your page, you can revisit the page generator and switch to another theme. Your Page content will be preserved.</p>

<h3>
<a id="convolutional-network" class="anchor" href="#convolutional-network" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Convolutional network</h3>

<h2>
<a id="lets-make-a-first-model" class="anchor" href="#lets-make-a-first-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Let's make a first model</h2>

<p>We will start to fiddle with the different layers available. This is where <code>nolearn</code> really shines as it allows a clear and easy way to implement your net. First we need to import a few modules:</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> lasagne
<span class="pl-k">from</span> lasagne <span class="pl-k">import</span> layers <span class="pl-c">#this allows us to use each pre-defined layer.</span>
<span class="pl-k">from</span> nolearn.lasagne <span class="pl-k">import</span> NeuralNet <span class="pl-c">#used to implement your net in a very simple fashion.</span></pre></div>

<p>We need to initialize our network just like a variable, let's name it <code>net0</code>. The initialization is divided in two steps:</p>

<ul>
<li>Describe the network architecture (i.e: layer composition)</li>
<li>Initialize each layer's parameters</li>
</ul>

<div class="highlight highlight-source-python"><pre>net0 <span class="pl-k">=</span> NeuralNet(
    <span class="pl-v">layers</span> <span class="pl-k">=</span> [(<span class="pl-s"><span class="pl-pds">'</span>layer1_name<span class="pl-pds">'</span></span>, layers.InsertLayerType),
              (<span class="pl-s"><span class="pl-pds">'</span>layer2_name<span class="pl-pds">'</span></span>, layers.InsertLayerType),
              (<span class="pl-c1">...</span>)],
    <span class="pl-v">layer1_param1</span> <span class="pl-k">=</span> <span class="pl-c1">...</span>, <span class="pl-v">layer1_param2</span> <span class="pl-k">=</span> <span class="pl-c1">...</span>,
    <span class="pl-v">layer2_param1</span> <span class="pl-k">=</span> <span class="pl-c1">...</span>, <span class="pl-v">layer2_param2</span> <span class="pl-k">=</span> <span class="pl-c1">...</span>,
)</pre></div>

<p><strong>Bear in mind that you need to use the pre-defined parameters name or an error will occur</strong></p>

<p>You are probably not yet familiar with each different layers, no worries we are going to cover that right now. Every single available layer is already implemented in Lasagne and that's good news. Despite being young its documentation is rich and the community is helpful, the following link will get you to the <a href="https://lasagne.readthedocs.io/en/latest/modules/layers.html">Lasagne official documentation</a> where each layer is described.</p>

<p>Now that we have the doc opened we can start playing with <code>net0</code> :</p>

<div class="highlight highlight-source-python"><pre><span class="pl-c">#from lasagne.updates import sgd</span>

net0 <span class="pl-k">=</span> NeuralNet(
    <span class="pl-v">layers</span> <span class="pl-k">=</span> [(<span class="pl-s"><span class="pl-pds">'</span>input<span class="pl-pds">'</span></span>, layers.InputLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>conv1<span class="pl-pds">'</span></span>, layers.Conv2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>conv2<span class="pl-pds">'</span></span>, layers.Conv2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>output<span class="pl-pds">'</span></span>, layers.DenseLayer)
    ],
    <span class="pl-v">input_shape</span> <span class="pl-k">=</span> (<span class="pl-c1">None</span>, <span class="pl-c1">1</span>, <span class="pl-c1">28</span>, <span class="pl-c1">28</span>), 

    <span class="pl-v">conv1_num_filters</span> <span class="pl-k">=</span>  <span class="pl-c1">8</span>,  <span class="pl-v">conv1_filter_size</span> <span class="pl-k">=</span> (<span class="pl-c1">3</span>, <span class="pl-c1">3</span>),
    <span class="pl-v">conv2_num_filters</span> <span class="pl-k">=</span>  <span class="pl-c1">16</span>, <span class="pl-v">conv2_filter_size</span> <span class="pl-k">=</span> (<span class="pl-c1">2</span>, <span class="pl-c1">2</span>),

    <span class="pl-v">output_num_units</span> <span class="pl-k">=</span> <span class="pl-c1">10</span>, <span class="pl-v">output_nonlinearity</span> <span class="pl-k">=</span> lasagne.nonlinearities.softmax,

    <span class="pl-c">#update = sgd,</span>
    <span class="pl-c">#update_learning_rate = 0.01,</span>

    <span class="pl-v">max_epochs</span> <span class="pl-k">=</span> <span class="pl-c1">20</span>,
    <span class="pl-v">verbose</span> <span class="pl-k">=</span> <span class="pl-c1">1</span>,
)</pre></div>

<p>So what do we have here...</p>

<ul>
<li>
<code>input_shape</code> describes the data processed by the network. Later on we will use a handwritten digit database named MNIST (you can find more about it <a href="http://yann.lecun.com/exdb/mnist/">here</a>). Each digit is a 28x28 1-dimensionnal image (grayscale so only 1 color channel).<br>
NB: the first parameter indicates the size of the batch, <code>None</code> means it is not fixed at compile time, thus <code>nolearn</code> will figure the value on its own.</li>
<li>
<code>conv_num_filters</code> is the amount of filters (or kernels) you will use and <code>conv_filter_size</code> specifies their size.</li>
<li>
<code>output_num_units</code> means that you sub-sample your data down to 10 images.</li>
<li>
<code>output_nonlinearity</code> is the activation function used, here it's the maximum (most commonly used for classification problems).</li>
</ul>

<p>Picking the proper sizes seems a little mystical but we will get to that. A nice visualization of the filtering process can be found in a JavaScript implementation <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html">here</a> (credits to <a href="https://github.com/Karpathy" class="user-mention">@Karpathy</a>).</p>

<p>Now that we have a functional implementation we have to train it !</p>

<h2>
<a id="loading-a-data-set" class="anchor" href="#loading-a-data-set" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Loading a data-set</h2>

<p>We are going to use the set provided in the <a href="https://www.kaggle.com/c/digit-recognizer">Kaggle MNIST competition</a>.</p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> pandas <span class="pl-k">as</span> pd
<span class="pl-k">import</span> numpy <span class="pl-k">as</span> np

<span class="pl-c"># The competition datafiles are in the directory ../input</span>
<span class="pl-c"># Read training and test data files</span>
train <span class="pl-k">=</span> pd.read_csv(<span class="pl-s"><span class="pl-pds">"</span>input/train.csv<span class="pl-pds">"</span></span>)
test  <span class="pl-k">=</span> pd.read_csv(<span class="pl-s"><span class="pl-pds">"</span>input/test.csv<span class="pl-pds">"</span></span>)

train_images <span class="pl-k">=</span> train.iloc[:,<span class="pl-c1">1</span>:].values
train_labels <span class="pl-k">=</span> train[[<span class="pl-c1">0</span>]].values.ravel()</pre></div>

<p>Once the data-set is loaded we have to add a few lines to start the training: <code>trainX</code> is the data used to learn and <code>trainY</code> the expected result. This method is called « supervised learning ».</p>

<div class="highlight highlight-source-python"><pre>net0 <span class="pl-k">=</span> NeuralNet(
    <span class="pl-c">#...</span>
)
<span class="pl-c"># Reshape and normalize training data</span>
trainX <span class="pl-k">=</span> train_images.reshape(train.shape[<span class="pl-c1">0</span>], <span class="pl-c1">1</span>, <span class="pl-c1">28</span>, <span class="pl-c1">28</span>).astype(np.float32)
trainX <span class="pl-k">/=</span> <span class="pl-c1">255.0</span>

<span class="pl-c"># Reshape and normalize test data</span>
testX <span class="pl-k">=</span> test.values.reshape(test.shape[<span class="pl-c1">0</span>], <span class="pl-c1">1</span>, <span class="pl-c1">28</span>, <span class="pl-c1">28</span>).astype(np.float32)
testX <span class="pl-k">/=</span> <span class="pl-c1">255.0</span>

net0.fit(trainX, trainY)</pre></div>

<p><strong>Uncomment the lines in</strong> <code>net0</code> <strong>if you want to test it yourself</strong></p>

<p>This is the results after 20 epochs (the input display origins from <code>verbose=1</code>):</p>

<div class="highlight highlight-source-shell"><pre><span class="pl-c"># Neural Network with 100618 learnable parameters</span>

<span class="pl-c">## Layer information</span>

  <span class="pl-c">#  name    size</span>
---  ------  --------
  0  input   1x28x28
  1  conv1   8x26x26
  2  conv2   16x25x25
  3  output  10

  epoch    train loss    valid loss    train/val    valid acc  dur
-------  ------------  ------------  -----------  -----------  ------
      1       5.16510       0.42123     12.26191      0.86043  10.53s
      2       0.35298       0.30983      1.13928      0.89896  10.58s
      3       0.27154       0.27564      0.98513      0.91174  10.55s
      4       0.23111       0.26905      0.85899      0.91618  10.59s
      5       0.20591       0.26782      0.76883      0.91595  10.71s
      6       0.18814       0.27432      0.68583      0.91678  10.70s
      7       0.17368       0.27859      0.62342      0.91891  10.56s
      8       0.16238       0.28430      0.57115      0.91926  10.69s
      9       0.15185       0.30812      0.49283      0.91541  10.67s
     10       0.14281       0.32322      0.44183      0.91465  10.61s
     11       0.13528       0.33334      0.40583      0.91328  10.59s
     12       0.12717       0.33398      0.38077      0.91358  10.66s
     13       0.12007       0.33142      0.36229      0.91678  10.63s
     14       0.11403       0.33387      0.34153      0.91666  10.61s
     15       0.10853       0.34899      0.31097      0.91571  10.56s
     16       0.10315       0.36111      0.28566      0.91334  10.58s
     17       0.10049       0.35958      0.27947      0.91494  10.56s
     18       0.09618       0.37745      0.25481      0.91370  10.58s
     19       0.09219       0.38261      0.24093      0.91322  10.63s
     20       0.08899       0.41470      0.21458      0.91062  10.68s</pre></div>

<p>Human performance would be above 98%, besides a network is considered industry-viable if its accuracy is at least 95%. We're not there yet but <code>net0</code> is really basic.</p>

<h2>
<a id="choosing-a-learning-method" class="anchor" href="#choosing-a-learning-method" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Choosing a learning method</h2>

<p>The following graph is a comparison of the commonly used learning methods:</p>

<p><img src="https://cloud.githubusercontent.com/assets/19994887/16196236/91019214-36fd-11e6-9e9b-1113334e7261.png" alt="learningmethod"></p>

<p>Our network will use <code>nesterov momentum</code>, it has good performance and allows us to tweak the <code>learning rate</code> and <code>momentum</code> as the training occurs (better optimization potential).</p>

<h2>
<a id="building-a-proper-architecture" class="anchor" href="#building-a-proper-architecture" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Building a proper architecture</h2>

<p>To obtain better results we're going to need more than convolution layers. Most ConvNets are constituted by a successsion of layers as follows (see <a href="http://cs231n.github.io/convolutional-networks/#layerpat">the course CS231n from Stanford University</a>):</p>

<p><code>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC</code></p>

<ul>
<li>Input Layer</li>
<li>Convolution Layer + ReLu</li>
<li>Pooling Layer</li>
<li>(...)Repeat</li>
<li>Fully Connected Layer (aka Hidden Layers)</li>
<li>Output Layer</li>
</ul>

<p>Considering all this we will add a few layers to <code>net0</code>:</p>

<div class="highlight highlight-source-python"><pre>net1 <span class="pl-k">=</span> NeuralNet(
    <span class="pl-v">layers</span> <span class="pl-k">=</span> [(<span class="pl-s"><span class="pl-pds">'</span>input<span class="pl-pds">'</span></span>, layers.InputLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>conv1<span class="pl-pds">'</span></span>, layers.Conv2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>pool1<span class="pl-pds">'</span></span>, layers.MaxPool2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>conv2<span class="pl-pds">'</span></span>, layers.Conv2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>pool2<span class="pl-pds">'</span></span>, layers.MaxPool2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>hidden1<span class="pl-pds">'</span></span>, layers.DenseLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>hidden2<span class="pl-pds">'</span></span>, layers.DenseLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>output<span class="pl-pds">'</span></span>, layers.DenseLayer)
              ],
              <span class="pl-c"># (...)</span>
)</pre></div>

<div class="highlight highlight-source-shell"><pre><span class="pl-c"># Neural Network with 38186 learnable parameters</span>

  epoch    train loss    valid loss    train/val    valid acc  dur
-------  ------------  ------------  -----------  -----------  -----
      1       1.02026       0.25914      3.93707      0.91891  7.89s
     ..       .......       .......      .......      .......  .....
     10       0.05761       0.08531      0.67536      0.97561  7.88s
     ..       .......       .......      .......      .......  .....
     20       0.02686       0.10420      0.25778      0.97182  7.91s
     ..       .......       .......      .......      .......  .....
     30       0.01065       0.11439      0.09308      0.97691  8.01s
     ..       .......       .......      .......      .......  .....
     40       0.00297       0.14241      0.02082      0.97573  8.04s
     ..       .......       .......      .......      .......  .....
     50       0.00100       0.16171      0.00615      0.97615  7.99s</pre></div>

<p>We notice a few things:</p>

<ul>
<li>Lowered amount of learnable parameters (due to the pooling layers).</li>
<li>The network is a lot faster (consequence of the above). The 2 seconds difference might not feel important to you but a deeper network can be trained for over 3000 epochs. </li>
<li>Much better accuracy &gt;97%.</li>
</ul>

<p>Let's trace a graph using <code>pyplot</code> to compare our results. </p>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> cPickle <span class="pl-k">as</span> pickle
<span class="pl-c"># (...)</span>
<span class="pl-c"># Saving the net with cPickle to load it back later.</span>
<span class="pl-k">with</span> <span class="pl-c1">open</span>(<span class="pl-s"><span class="pl-pds">'</span>net0.pickle<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>wb<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> f:
   pickle.dump(net0, f, <span class="pl-k">-</span><span class="pl-c1">1</span>)</pre></div>

<div class="highlight highlight-source-python"><pre><span class="pl-k">import</span> matplotlib.pyplot <span class="pl-k">as</span> plt
<span class="pl-k">try</span>:
    <span class="pl-k">with</span> <span class="pl-c1">open</span>(<span class="pl-s"><span class="pl-pds">'</span>net1.pickle<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>rb<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> f1, <span class="pl-c1">open</span>(<span class="pl-s"><span class="pl-pds">'</span>net2.pickle<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>rb<span class="pl-pds">'</span></span>) <span class="pl-k">as</span> f2:
        net1 <span class="pl-k">=</span> cPickle.load(f1)
        net2 <span class="pl-k">=</span> cPickle.load(f2)

        train_loss_net1 <span class="pl-k">=</span> np.array([i[<span class="pl-s"><span class="pl-pds">"</span>train_loss<span class="pl-pds">"</span></span>]  <span class="pl-k">for</span> i <span class="pl-k">in</span> net1.train_history_])
        valid_loss_net1 <span class="pl-k">=</span> np.array([i[<span class="pl-s"><span class="pl-pds">"</span>valid_loss<span class="pl-pds">"</span></span>]  <span class="pl-k">for</span> i <span class="pl-k">in</span> net1.train_history_])  

        train_loss_net2 <span class="pl-k">=</span> np.array([i[<span class="pl-s"><span class="pl-pds">"</span>train_loss<span class="pl-pds">"</span></span>]  <span class="pl-k">for</span> i <span class="pl-k">in</span> net2.train_history_])
        valid_loss_net2 <span class="pl-k">=</span> np.array([i[<span class="pl-s"><span class="pl-pds">"</span>valid_loss<span class="pl-pds">"</span></span>]  <span class="pl-k">for</span> i <span class="pl-k">in</span> net2.train_history_])

        plt.plot(train_loss_net1, <span class="pl-s"><span class="pl-pds">'</span>b<span class="pl-pds">'</span></span>, <span class="pl-v">linewidth</span><span class="pl-k">=</span><span class="pl-c1">2</span>, <span class="pl-v">label</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Net1 valid/loss<span class="pl-pds">"</span></span>)
        plt.plot(train_loss_net2, <span class="pl-s"><span class="pl-pds">'</span>b--<span class="pl-pds">'</span></span>, <span class="pl-v">linewidth</span><span class="pl-k">=</span><span class="pl-c1">2</span>, <span class="pl-v">label</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Net2 valid/loss<span class="pl-pds">"</span></span>)
        plt.plot(valid_loss_net1, <span class="pl-s"><span class="pl-pds">'</span>r<span class="pl-pds">'</span></span>, <span class="pl-v">linewidth</span><span class="pl-k">=</span><span class="pl-c1">2</span>, <span class="pl-v">label</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Net1 train/loss<span class="pl-pds">"</span></span>)
        plt.plot(valid_loss_net2, <span class="pl-s"><span class="pl-pds">'</span>r--<span class="pl-pds">'</span></span>, <span class="pl-v">linewidth</span><span class="pl-k">=</span><span class="pl-c1">2</span>, <span class="pl-v">label</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">"</span>Net2 train/loss<span class="pl-pds">"</span></span>)

        plt.grid()
        plt.legend()  
        plt.xlabel(<span class="pl-s"><span class="pl-pds">"</span>Epoch<span class="pl-pds">"</span></span>)
        plt.ylabel(<span class="pl-s"><span class="pl-pds">"</span>Loss<span class="pl-pds">"</span></span>)
        plt.ylim(<span class="pl-c1">1e-2</span>, <span class="pl-c1">1e-1</span>)
        plt.yscale(<span class="pl-s"><span class="pl-pds">"</span>log<span class="pl-pds">"</span></span>)
        plt.show()

<span class="pl-k">except</span> <span class="pl-c1">IOError</span> <span class="pl-k">as</span> e:
    <span class="pl-c1">print</span> <span class="pl-s"><span class="pl-pds">'</span>Operation failed: <span class="pl-c1">%s</span><span class="pl-pds">'</span></span> <span class="pl-k">%</span> e.strerror
</pre></div>

<h2>
<a id="how-to-tune-the-number-of-kernels-filters-" class="anchor" href="#how-to-tune-the-number-of-kernels-filters-" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>How to tune the number of kernels (filters) ?</h2>

<p>This is a tricky part, from what I have read and learned it is mostly done empirically. A team of Google researchers tried to explain the importance of proper initialization and gave some guidelines, you can read their work here <a href="http://arxiv.org/abs/1512.00567">Rethinking the Inception Architecture for Computer Vision</a> (not beginner friendly).</p>

<p>For the moment a simple way of proceeding is to enable <code>verbose=2</code>. This will give us more information about our network's learning capacity of the coverage of its data. Later on we will try to experiment a way of finding better values (see part 2).</p>

<div class="highlight highlight-source-shell"><pre><span class="pl-c"># Neural Network with 69818 learnable parameters</span>

<span class="pl-c">## Layer information</span>

name     size        total    cap.Y    cap.X    cov.Y    cov.X
-------  --------  -------  -------  -------  -------  -------
input    1x24x24       576   100.00   100.00   100.00   100.00
conv1    16x22x22     7744   100.00   100.00    12.50    12.50
conv2    16x21x21     7056    50.00    50.00    16.67    16.67
pool1    16x10x10     1600    50.00    50.00    16.67    16.67
conv3    32x9x9       2592    66.67    66.67    25.00    25.00
conv4    32x8x8       2048    50.00    50.00    33.33    33.33
pool2    32x4x4        512    50.00    50.00    33.33    33.33
hidden1  100           100   100.00   100.00   100.00   100.00
hidden2  100           100   100.00   100.00   100.00   100.00
output   10             10   100.00   100.00   100.00   100.00

Explanation
    X, Y:    image dimensions
    cap.:    learning capacity
    cov.:    coverage of image
    magenta: capacity too low (<span class="pl-k">&lt;</span>1/6)
    cyan:    image coverage too high (<span class="pl-k">&gt;</span>100%)
    red:     capacity too low and coverage too high
</pre></div>

<h2>
<a id="the-overfitting-problem" class="anchor" href="#the-overfitting-problem" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The overfitting problem</h2>

<div class="highlight highlight-source-python"><pre>net2 <span class="pl-k">=</span> NeuralNet(
    <span class="pl-v">layers</span> <span class="pl-k">=</span> [(<span class="pl-s"><span class="pl-pds">'</span>input<span class="pl-pds">'</span></span>, layers.InputLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>conv1<span class="pl-pds">'</span></span>, layers.Conv2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>pool1<span class="pl-pds">'</span></span>, layers.MaxPool2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>conv2<span class="pl-pds">'</span></span>, layers.Conv2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>pool2<span class="pl-pds">'</span></span>, layers.MaxPool2DLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>hidden1<span class="pl-pds">'</span></span>, layers.DenseLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>hidden2<span class="pl-pds">'</span></span>, layers.DenseLayer),
              (<span class="pl-s"><span class="pl-pds">'</span>output<span class="pl-pds">'</span></span>, layers.DenseLayer)
              ],
    <span class="pl-c"># Each digit is a 28x28 1-Dimensionnal image,</span>
    <span class="pl-c"># We crop each image by 2 pixels in width and height.</span>
    <span class="pl-v">input_shape</span> <span class="pl-k">=</span> (<span class="pl-c1">None</span>, <span class="pl-c1">1</span>, <span class="pl-c1">26</span>, <span class="pl-c1">26</span>),

    <span class="pl-v">conv1_num_filters</span> <span class="pl-k">=</span>  <span class="pl-c1">13</span>, <span class="pl-v">conv1_filter_size</span> <span class="pl-k">=</span> (<span class="pl-c1">3</span>, <span class="pl-c1">3</span>),
    <span class="pl-v">conv1_nonlinearity</span> <span class="pl-k">=</span> lasagne.nonlinearities.rectify,
    <span class="pl-v">pool1_pool_size</span> <span class="pl-k">=</span> (<span class="pl-c1">2</span>, <span class="pl-c1">2</span>),

    <span class="pl-v">conv2_num_filters</span> <span class="pl-k">=</span> <span class="pl-c1">26</span>, <span class="pl-v">conv2_filter_size</span> <span class="pl-k">=</span> (<span class="pl-c1">2</span>, <span class="pl-c1">2</span>),
    <span class="pl-v">conv2_nonlinearity</span> <span class="pl-k">=</span> lasagne.nonlinearities.rectify,
    <span class="pl-v">pool2_pool_size</span> <span class="pl-k">=</span> (<span class="pl-c1">2</span>, <span class="pl-c1">2</span>),

    <span class="pl-v">hidden1_num_units</span> <span class="pl-k">=</span> <span class="pl-c1">128</span>, <span class="pl-v">hidden2_num_units</span> <span class="pl-k">=</span> <span class="pl-c1">128</span>,
    <span class="pl-v">output_num_units</span> <span class="pl-k">=</span> <span class="pl-c1">10</span>,  <span class="pl-v">output_nonlinearity</span> <span class="pl-k">=</span> lasagne.nonlinearities.softmax,

    <span class="pl-c"># Learning method.</span>
    <span class="pl-v">update</span> <span class="pl-k">=</span> nesterov_momentum,
    <span class="pl-v">update_learning_rate</span> <span class="pl-k">=</span> <span class="pl-c1">0.01</span>,
    <span class="pl-v">update_momentum</span> <span class="pl-k">=</span> <span class="pl-c1">0.9</span>,    

    <span class="pl-c"># Calls the class CropBatchIterator as the training occurs</span>
    <span class="pl-c"># Doesn't affect GPU performance as the CPU deals with the</span>
    <span class="pl-c"># cropping operation.</span>
    <span class="pl-v">batch_iterator_train</span> <span class="pl-k">=</span> CropBatchIterator(<span class="pl-v">batch_size</span><span class="pl-k">=</span><span class="pl-c1">200</span>),
    <span class="pl-v">batch_iterator_test</span> <span class="pl-k">=</span> CropBatchIterator(<span class="pl-v">batch_size</span><span class="pl-k">=</span><span class="pl-c1">200</span>),

    <span class="pl-v">regression</span> <span class="pl-k">=</span> <span class="pl-c1">False</span>,
    <span class="pl-v">max_epochs</span> <span class="pl-k">=</span> <span class="pl-c1">100</span>,

    <span class="pl-c"># Deeper layer information (learning capacity, coverage...)</span>
    <span class="pl-v">verbose</span> <span class="pl-k">=</span> <span class="pl-c1">2</span>, 

)</pre></div>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors and Contributors</h3>

<p>You can <a href="https://help.github.com/articles/basic-writing-and-formatting-syntax/#mentioning-users-and-teams" class="user-mention">@mention</a> a GitHub username to generate a link to their profile. The resulting <code>&lt;a&gt;</code> element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (<a href="https://github.com/defunkt" class="user-mention">@defunkt</a>), PJ Hyett (<a href="https://github.com/pjhyett" class="user-mention">@pjhyett</a>), and Tom Preston-Werner (<a href="https://github.com/mojombo" class="user-mention">@mojombo</a>) founded GitHub.</p>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Support or Contact</h3>

<p>Having trouble with Pages? Check out our <a href="https://help.github.com/pages">documentation</a> or <a href="https://github.com/contact">contact support</a> and we’ll help you sort it out.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/jcouvy/convnet-nolearn">Testing project site</a> is maintained by <a href="https://github.com/jcouvy">jcouvy</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
